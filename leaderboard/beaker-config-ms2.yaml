version: v2-alpha
description: MS2 evaluation
tasks:
  # We only have one step in our experiment, so there's only one entry in this list
  - name: evaluation
    image:
      beaker: lucyw/s2-leaderboard-eval-mslr-2
    command: [
        "conda", "run", "-n", "mslr",
        "python", "/app/evaluator/evaluator.py"
    ]
    arguments: [
      "--targets",
      "/app/data/test-targets-ms2.csv",
      "--predictions",
      "/app/data/predicted.csv",
      "--output",
      "/app/output/metrics.json",
      "--ei_param_file",
      "/app/models/bert_pipeline_8samples.json",
      "--ei_model_dir",
      "/app/models/evidence_inference_models"
    ]
    result:
      # Beaker will capture anything that's written to this location and store it in the results
      # dataset.
      path: /app/output
    resources:
      gpuCount: 1
    context:
      cluster: leaderboard/CPU
      priority: normal